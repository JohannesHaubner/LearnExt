Network: MaskNet(
  (network): Sequential(
    (0): PrependModule()
    (1): MLP_BN(
      (activation): ReLU()
      (bn): BatchNorm1d(3935, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (layers): Sequential(
        (0): BatchNorm1d(3935, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=64, bias=True)
        (4): ReLU()
        (5): Linear(in_features=64, out_features=64, bias=True)
        (6): ReLU()
        (7): Linear(in_features=64, out_features=64, bias=True)
        (8): ReLU()
        (9): Linear(in_features=64, out_features=64, bias=True)
        (10): ReLU()
        (11): Linear(in_features=64, out_features=64, bias=True)
        (12): ReLU()
        (13): Linear(in_features=64, out_features=64, bias=True)
        (14): ReLU()
        (15): Linear(in_features=64, out_features=64, bias=True)
        (16): ReLU()
        (17): Linear(in_features=64, out_features=64, bias=True)
        (18): ReLU()
        (19): Linear(in_features=64, out_features=64, bias=True)
        (20): ReLU()
        (21): Linear(in_features=64, out_features=2, bias=True)
      )
    )
  )
  (base): TrimModule()
  (mask): TensorModule()
) 
Cost function: L1Loss()
Validation Cost function: MSELoss()
Optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 6.25e-05
    maximize: False
    weight_decay: 0.01
)
Scheduler: ReduceLROnPlateau: 
	{'factor': 0.5, 'min_lrs': [0], 'patience': 10, 'verbose': False, 'cooldown': 0, 'cooldown_counter': 0, 'mode': 'min', 'threshold': 0.0001, 'threshold_mode': 'rel', 'best': 1.0077744150294166e-07, 'num_bad_epochs': 8, 'mode_worse': inf, 'eps': 1e-08, 'last_epoch': 500, '_last_lr': [6.25e-05]}
Final train loss: 0.001045410826918669
Final val loss: 1.0929575466889219e-07
Final lr: 6.25e-05